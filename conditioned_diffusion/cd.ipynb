{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "device = torch.device(\n",
    "        'cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Small_F_net(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dnn = nn.Sequential(nn.Linear(self.z_dim, self.latent_dim),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(self.latent_dim, self.latent_dim),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(self.latent_dim, self.z_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.dnn(x)\n",
    "        return f\n",
    "\n",
    "\n",
    "class Conditioned_Diffusion:\n",
    "\n",
    "    def __init__(self, num_interval, num_obs, beta=10.0, T=1.0):\n",
    "\n",
    "        self.num_interval = num_interval\n",
    "        self.num_obs = num_obs\n",
    "        self.T = T\n",
    "        self.stepsize = T / num_interval\n",
    "        self.beta = beta\n",
    "\n",
    "    def drift(self, u):\n",
    "        return self.beta * u * (1 - u**2) / (1 + u**2)\n",
    "\n",
    "    def generate_path(self, sigma=0.1, batch_size=1, xi=None):\n",
    "        if isinstance(xi, torch.Tensor):\n",
    "            batch_size = xi.shape[0]\n",
    "        else:\n",
    "            xi = torch.randn(batch_size, self.num_interval).to(device)\n",
    "        x = torch.zeros(batch_size, self.num_interval + 1).to(device)\n",
    "        u = torch.zeros(batch_size, self.num_interval + 1).to(device)\n",
    "        for k in range(self.num_interval):\n",
    "            temp = np.sqrt(self.stepsize) * xi[:, k]\n",
    "            x[:, k + 1] = x[:, k] + temp\n",
    "            u[:, k + 1] = u[:, k] + self.stepsize * self.drift(u[:, k]) + temp\n",
    "\n",
    "        noise = torch.randn(batch_size, self.num_obs).to(device) * sigma\n",
    "        obs_interval = self.num_interval / self.num_obs\n",
    "        y = u[:,\n",
    "              np.arange(obs_interval, self.num_interval +\n",
    "                        1, obs_interval)] + noise\n",
    "\n",
    "        return xi, x, u, y\n",
    "\n",
    "    def loglikelihood(self, xi, y, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            xi: inputs, with shape (batch, num_interval)\n",
    "            y: observations, with shape (batch, num_obs)\n",
    "            sigma: standard deviation of observation noises\n",
    "        \"\"\"\n",
    "\n",
    "        batch = xi.shape[0]\n",
    "        uk = torch.zeros(batch).to(device)\n",
    "        logll = torch.zeros(batch).to(device)\n",
    "        obs_interval = self.num_interval / self.num_obs\n",
    "\n",
    "        for k in range(self.num_interval):\n",
    "            vk = uk + self.stepsize * self.drift(uk) + np.sqrt(\n",
    "                self.stepsize) * xi[:, k]\n",
    "            if k > 0 and k % obs_interval == 0:\n",
    "                logll = logll - 0.5 * (y[:, int(k / obs_interval) - 1] -\n",
    "                                       uk)**2 / sigma**2\n",
    "            uk = vk\n",
    "        logll = logll - 0.5 * (y[:, -1] - uk)**2 / sigma**2\n",
    "        return logll\n",
    "    \n",
    "    def posterior_score(self, xi, y, sigma=0.1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            xi: inputs, with shape (batch, num_interval)\n",
    "            y: observations, with shape (batch, num_obs)\n",
    "            sigma: standard deviation of observation noises\n",
    "        \"\"\"\n",
    "\n",
    "        prior_score = -xi\n",
    "        dup_xi = xi.view(xi.shape[0], -1)\n",
    "        dup_xi.requires_grad_(True)\n",
    "        logll = self.loglikelihood(dup_xi, y, sigma)\n",
    "        likelihood_score = autograd.grad(logll.sum(), dup_xi)[0]\n",
    "\n",
    "        score = prior_score + likelihood_score\n",
    "        return score\n",
    "\n",
    "    def evaluation(self, xi):\n",
    "        _, _, u, _ = self.generate_path(xi=xi.detach())\n",
    "        u = u.detach().cpu()[:, 1:]\n",
    "        u_sgld_true = torch.load(\"cd_sgld_u_12345.pt\").detach().cpu()\n",
    "        mmd = MMDStatistic(u.shape[0], u_sgld_true.shape[0])\n",
    "        logMMD = np.log(mmd(u, u_sgld_true, [0.1, 1]))\n",
    "\n",
    "        return logMMD\n",
    "\n",
    "\n",
    "class MMDStatistic:\n",
    "    def __init__(self, n_1, n_2):\n",
    "        self.n_1 = n_1\n",
    "        self.n_2 = n_2\n",
    "\n",
    "        self.a00 = 1. / (n_1 * (n_1 - 1))\n",
    "        self.a11 = 1. / (n_2 * (n_2 - 1))\n",
    "        self.a01 = - 1. / (n_1 * n_2)\n",
    "\n",
    "    def __call__(self, sample_1, sample_2, alphas, ret_matrix=False):\n",
    " \n",
    "        sample_12 = torch.cat((sample_1, sample_2), 0)\n",
    "        distances = squareform(pdist(sample_12))\n",
    "\n",
    "        kernels = None\n",
    "        for alpha in alphas:\n",
    "            kernels_a = np.exp(- alpha * distances ** 4)\n",
    "            if kernels is None:\n",
    "                kernels = kernels_a\n",
    "            else:\n",
    "                kernels += kernels_a\n",
    "\n",
    "        k_1 = kernels[:self.n_1, :self.n_1]\n",
    "        k_2 = kernels[self.n_1:, self.n_1:]\n",
    "        k_12 = kernels[:self.n_1, self.n_1:]\n",
    "\n",
    "        mmd = (2 * self.a01 * k_12.sum() +\n",
    "               self.a00 * (k_1.sum() - np.trace(k_1)) +\n",
    "               self.a11 * (k_2.sum() - np.trace(k_2)))\n",
    "        if ret_matrix:\n",
    "            return mmd, kernels\n",
    "        else:\n",
    "            return mmd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle VI class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParVI():\n",
    "\n",
    "    def __init__(self, target_score, dim, latent_dim):\n",
    "        self.target_score = target_score\n",
    "        self.dim = dim\n",
    "        self.f_net = Small_F_net(dim, latent_dim).to(device)\n",
    "\n",
    "    def precondition_g(self, x, alpha=1):\n",
    "        # beta = 0.1\n",
    "        beta = 1.0\n",
    "        dup_x = x.reshape(-1, self.dim)\n",
    "        dup_x.requires_grad_(True)\n",
    "        n = dup_x.shape[0]\n",
    "        fx = self.f_net(dup_x)\n",
    "        H = 1 / torch.var(dup_x, dim=0)**alpha\n",
    "        H = H.repeat(n, 1).detach()\n",
    "        self.H = beta * H + (1 - beta) * self.H\n",
    "        h_norm = 0.5 * torch.sum(self.H * fx * fx) / n\n",
    "\n",
    "        return h_norm\n",
    "\n",
    "    def svgd_kernel(self, x, h=-1):\n",
    "        x = x.detach().numpy()\n",
    "        sq_dist = pdist(x)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0:  # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)\n",
    "            h = np.sqrt(0.5 * h / np.log(x.shape[0] + 1))\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp(-pairwise_dists / h**2 / 2)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, x)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(x.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:, i] + np.multiply(x[:, i], sumkxy)\n",
    "        dxkxy = dxkxy / (h**2)\n",
    "        return torch.from_numpy(Kxy).float(), torch.from_numpy(dxkxy).float()\n",
    "\n",
    "    def gssm(self,\n",
    "             samples,\n",
    "             n_particles=1,\n",
    "             g_fn='p_norm',\n",
    "             p=2,\n",
    "             precond_alpha=1.0):\n",
    "        dup_samples = samples.view(-1, self.dim)\n",
    "        dup_samples.requires_grad_(True)\n",
    "\n",
    "        score = self.target_score(dup_samples)\n",
    "        f = self.f_net(dup_samples)\n",
    "\n",
    "        loss1 = torch.sum(f * score, dim=-1).mean()\n",
    "        loss2 = torch.zeros(samples.shape[0]).to(device)\n",
    "        for _ in range(n_particles):\n",
    "            vectors = torch.randn_like(dup_samples).to(device)\n",
    "            gradv = torch.sum(f * vectors)\n",
    "            grad2 = autograd.grad(gradv,\n",
    "                                  dup_samples,\n",
    "                                  create_graph=True,\n",
    "                                  retain_graph=True)[0]\n",
    "            loss2 += torch.sum(vectors * grad2, dim=-1) / n_particles\n",
    "        loss2 = loss2.mean()\n",
    "\n",
    "        if g_fn == 'p_norm':\n",
    "            loss3 = torch.norm(f, p=p, dim=-1)**p / p\n",
    "            loss3 = loss3.mean()\n",
    "        elif g_fn == 'precondition':\n",
    "            loss3 = self.precondition_g(dup_samples, precond_alpha)\n",
    "\n",
    "        loss = loss1 + loss2 - loss3\n",
    "        return loss\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        f_opt,\n",
    "        g_fn,\n",
    "        sample_size,\n",
    "        p=2,\n",
    "        precond_alpha=1,\n",
    "        step_size=1e-3,\n",
    "        adagrad=False,\n",
    "        adaptive=False,\n",
    "        p_step=1e-4,\n",
    "        lb=1.1,\n",
    "        ub=6.0,\n",
    "        alpha=0.9,\n",
    "        n_epoch=2000,\n",
    "        f_iter=1,\n",
    "        pre_train_epoch=100,\n",
    "        check_frq=200,\n",
    "        evaluation=lambda x: 0,\n",
    "    ):\n",
    "\n",
    "        x = torch.randn(sample_size, self.dim).to(device)\n",
    "        xs = []\n",
    "        info = []\n",
    "        historical_grad = 0\n",
    "        fudge_factor = 1e-6\n",
    "        self.H = torch.zeros(sample_size, self.dim,\n",
    "                             requires_grad=False).to(device)\n",
    "\n",
    "        for i in range(pre_train_epoch):\n",
    "            dup_x = x.data\n",
    "            dup_x.requires_grad_(True)\n",
    "            f_loss = -self.gssm(dup_x, g_fn=g_fn, p=p)\n",
    "            f_opt.zero_grad()\n",
    "            f_loss.backward()\n",
    "            f_opt.step()\n",
    "\n",
    "        for ep in trange(n_epoch):\n",
    "            dup_x = x.data\n",
    "            dup_x.requires_grad_(True)\n",
    "\n",
    "            if g_fn == 'sgld':\n",
    "                noise = torch.randn_like(x).to(device)\n",
    "                x = x + step_size * self.target_score(dup_x) + np.sqrt(\n",
    "                    2 * step_size) * noise\n",
    "            else:\n",
    "                if g_fn == 'svgd':\n",
    "                    s = self.target_score(dup_x)\n",
    "                    kxy, dxkxy = self.svgd_kernel(x.cpu(), h=-1)\n",
    "                    kxy = kxy.to(device)\n",
    "                    dxkxy = dxkxy.to(device)\n",
    "                    v = (torch.matmul(kxy, s) + dxkxy) / sample_size\n",
    "                else:\n",
    "                    for i in range(f_iter):\n",
    "                        f_loss = -self.gssm(\n",
    "                            dup_x, g_fn=g_fn, p=p, precond_alpha=precond_alpha)\n",
    "                        f_opt.zero_grad()\n",
    "                        f_loss.backward()\n",
    "                        f_opt.step()\n",
    "\n",
    "                    v = self.f_net(x)\n",
    "\n",
    "                # adagrad\n",
    "                if adagrad:\n",
    "                    if ep == 0:\n",
    "                        historical_grad = historical_grad + v**2\n",
    "                    else:\n",
    "                        historical_grad = alpha * historical_grad + (\n",
    "                            1 - alpha) * v**2\n",
    "                    v = torch.divide(\n",
    "                        v, fudge_factor + torch.sqrt(historical_grad))\n",
    "\n",
    "                # update particles\n",
    "                x = x + step_size * v\n",
    "\n",
    "                # adaptive p\n",
    "                if g_fn == 'p_norm' and adaptive:\n",
    "                    # compute gradient of p\n",
    "                    v = abs(v).detach().cpu().numpy()**p\n",
    "                    grad_p = np.sum(v * (np.log(v + 1e-7) - 1),\n",
    "                                    axis=1).mean() / p**2\n",
    "                    grad_p = np.clip(grad_p, -0.1, 0.1)\n",
    "\n",
    "                    p += p_step * grad_p\n",
    "                    p = np.clip(p, lb, ub)\n",
    "\n",
    "            if (ep % check_frq == 0) or (ep == n_epoch - 1):\n",
    "                xs += [x]\n",
    "                info += [evaluation(x)]\n",
    "                print(info[-1])\n",
    "\n",
    "        return xs, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "np.random.seed(12345)\n",
    "\n",
    "num_interval = 100\n",
    "num_obs = 20\n",
    "beta = 10\n",
    "T = 1.0\n",
    "sigma = 0.1\n",
    "CD = Conditioned_Diffusion(num_interval, num_obs, beta, T)\n",
    "xi_true, x_true, u_true, y = CD.generate_path(sigma, batch_size=1)\n",
    "\n",
    "dim = num_interval\n",
    "latent_dim = 200\n",
    "sample_size = 1000\n",
    "\n",
    "\n",
    "def cd_score(x):\n",
    "    return CD.posterior_score(x, y, sigma)\n",
    "\n",
    "\n",
    "parvi = ParVI(cd_score, dim, latent_dim)\n",
    "f_opt = optim.Adam(parvi.f_net.parameters(), lr=1e-3)\n",
    "xi_ada, info_ada = parvi.sample(f_opt,\n",
    "                                'p_norm',\n",
    "                                sample_size,\n",
    "                                p=2.0,\n",
    "                                adaptive=True,\n",
    "                                p_step=3e-3,\n",
    "                                step_size=3e-3,\n",
    "                                n_epoch=500,\n",
    "                                f_iter=15,\n",
    "                                pre_train_epoch=100,\n",
    "                                check_frq=50,\n",
    "                                evaluation=CD.evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precondition\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "np.random.seed(12345)\n",
    "\n",
    "num_interval = 100\n",
    "num_obs = 20\n",
    "beta = 10\n",
    "T = 1.0\n",
    "sigma = 0.1\n",
    "CD = Conditioned_Diffusion(num_interval, num_obs, beta, T)\n",
    "xi_true, x_true, u_true, y = CD.generate_path(sigma, batch_size=1)\n",
    "\n",
    "dim = num_interval\n",
    "latent_dim = 200\n",
    "sample_size = 1000\n",
    "\n",
    "\n",
    "def cd_score(x):\n",
    "    return CD.posterior_score(x, y, sigma)\n",
    "\n",
    "\n",
    "parvi = ParVI(cd_score, dim, latent_dim)\n",
    "f_opt = optim.Adam(parvi.f_net.parameters(), lr=1e-3)\n",
    "xi_H2, info_H2 = parvi.sample(f_opt,\n",
    "                              'precondition',\n",
    "                              sample_size,\n",
    "                              p=2,\n",
    "                              precond_alpha=1,\n",
    "                              step_size=3e-3,\n",
    "                              n_epoch=500,\n",
    "                              f_iter=15,\n",
    "                              pre_train_epoch=100,\n",
    "                              check_frq=50,\n",
    "                              evaluation=CD.evaluation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
